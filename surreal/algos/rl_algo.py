# Copyright 2019 ducandu GmbH. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

from abc import ABCMeta, abstractmethod

from surreal.algos.algo import Algo


class RLAlgo(Algo, metaclass=ABCMeta):
    """
    A reinforcement learning Algo that interacts with an environment to learn how to "predict" actions such that
    the sum of received reward signals (coming from the environment) is maximized.
    See [1].
    [1] Reinforcement Learning - An Introduction, 2nd Edition; A.G. Barto, R.S. Sutton 2018
    """
    @abstractmethod
    def event_tick(self, env, time_steps, batch_positions, r, t, s_):
        """
        Tick event handler. Ticks are generated by the environment(s), which this Algo serves as a decision learner
        and maker. Somewhere inside this handler, the Algo is supposed to make an action decision - based on
        the environment's current or historic state(s) and rewards - and call back `env.act()` passing the
        Algo's resulting action decisions.

        Args:
            env (Env): The Env that triggered this tick.
            time_steps (int): The time steps (across all actors that share this Algo).
            batch_positions (List[int]): The positions in the actor-batch for which the tick has happened.
            r (np.ndarray): The batch of rewards received when reaching the next state (s').
            t (np.ndarray): The terminal signals received when reaching the next state (s').

            s_ (any): The batch of next states (s'). NOTE: If `t` is True for some s' in the batch, then s' is
                the first state of a new (reset) episode and `r` is the last reward received in the old
                episode.
        """
        raise NotImplementedError

    def event_episode_starts(self, env, time_steps, batch_positions, s):
        """
        Called whenever a new episode starts in the Env.

        Args:
            env (Env): The Env that triggered this event.
            time_steps (int): The time steps (across all actors that share this Algo).
            batch_positions (int): The position in the actor-batch at which a reset has happened.
            s (any): The new state (s) after a reset.
        """
        pass

    def event_episode_ends(self, env, time_steps, batch_position):
        """
        Called whenever an episode ended in the Env.

        Args:
            env (Env): The Env that triggered this event.
            time_steps (int): The time steps (across all actors that share this Algo).
            batch_position (int): The position in the actor-batch at which a terminal=True was observed.
            s (any): The terminal state (s') before(!) any reset.
        """
        pass
